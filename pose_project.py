# -*- coding: utf-8 -*-
"""pose_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1snqZDeJ7WmQ7oBunnqcDANwjVo8age8Q
"""

#load images from 300ws

import tensorflow_datasets as tfds
import tensorflow as tf

!pip install -q tensorflow-datasets
print(tf.VERSION)
print(tfds.list_builders())
im_300W_builder = tfds.builder('the300w_lp')

# Download the dataset
im_300W_builder.download_and_prepare()

# Construct a tf.data.Dataset
# ds = im_300W_builder.as_dataset(split='train')

# Get the `DatasetInfo` object, which contains useful information about the
# dataset and its features
info = im_300W_builder.info
print(info)

ds = im_300W_builder.as_dataset(split='train')

numpy_ds = tfds.as_numpy(ds)
img_data1 = next(numpy_ds)
# print(img_data1.keys())
img1 = img_data1.get('image')
# print(img1)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
from matplotlib import pyplot as plt
plt.imshow(img1)
plt.show()

print(img_data1.get('pose_params'))
pose_arr = img_data1.get('pose_params')[:3]
print((pose_arr))

ladmarks_2D_1 = img_data1.get('landmarks_2d')
ladmarks_3D_1 = img_data1.get('landmarks_3d')
# print(ladmarks_2D_1)
# print(type(ladmarks_2D_1))
# print(ladmarks_3D_1)

#get the data from the drive
from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np
import zipfile
import io
import csv
from matplotlib import pyplot as plt
import glob

# calc NME for landmark - base from https://github.com/protossw512/AdaptiveWingLoss/blob/d0bf27a09162d97443c9112b10d05b3256012e7a/utils/utils.py
def calc_NME(pred_landmarks, gt_landmarks, num_landmarks=68):
    '''
       Calculate total NME for a batch of data
       Args:
           pred_landmarks: list of tuples img name and points
           gt_landmarks: list of tuples img name and points
       Returns:
           nme: sum of nme for this batch
           nme_list: nme for each img with img name
    '''
    nme = 0
    nme_list = []

    for i in range(len(pred_landmarks)):
        #check how much landmarks was found for each img
        num_landmarks = len(pred_landmarks[i][1]) 
        if(num_landmarks != len(gt_landmarks[i][1])):
            nme = nme+1000 #add big error
            nme_list.appebd((gt_landmarks[i][0], 1000))
            continue

        pred_landmark = pred_landmarks[i][1] * 4.0
        gt_landmark = gt_landmarks[i][1]

        if num_landmarks == 68:
            left_eye = np.average(gt_landmark[36:42], axis=0)
            right_eye = np.average(gt_landmark[42:48], axis=0)
            norm_factor = np.linalg.norm(left_eye - right_eye)
            # norm_factor = np.linalg.norm(gt_landmark[36]- gt_landmark[45])
        elif num_landmarks == 98:
            norm_factor = np.linalg.norm(gt_landmark[60]- gt_landmark[72])
        elif num_landmarks == 19:
            left, top = gt_landmark[-2, :]
            right, bottom = gt_landmark[-1, :]
            norm_factor = math.sqrt(abs(right - left)*abs(top-bottom))
            gt_landmark = gt_landmark[:-2, :]
        elif num_landmarks == 29:
            # norm_factor = np.linalg.norm(gt_landmark[8]- gt_landmark[9])
            norm_factor = np.linalg.norm(gt_landmark[16]- gt_landmark[17])
        cur_nme = (np.sum(np.linalg.norm(pred_landmark - gt_landmark, axis=1)) / pred_landmark.shape[0]) / norm_factor
        nme += cur_nme
        nme_list.append((gt_landmarks[i][0],cur_nme))
    return nme, nme_list

# pts_loader provides a load() method to read data from .pts files of
# point clouds
#
# --------------------------------------------------------
# pts_loader
# Licensed under The MIT License [see LICENSE.md for details]
# Copyright (C) 2017 Samuel Albanie 
# --------------------------------------------------------
def load_get_landmark_was_pointed(path):
    """takes as input the path to a .pts and returns a list of 
	tuples of floats containing the points in in the form:
	[(x_0, y_0, z_0),
	 (x_1, y_1, z_1),
	 ...
	 (x_n, y_n, z_n)]"""
    with open(path) as f:
        rows = [rows.strip() for rows in f]
    
    """Use the curly braces to find the start and end of the point data""" 
    head = rows.index('{') + 1
    tail = rows.index('}')

    """Select the point data split into coordinates"""
    raw_points = rows[head:tail]
    coords_set = [point.split() for point in raw_points]

    """Convert entries from lists of strings to tuples of floats"""
    points = [tuple([float(point) for point in coords]) for coords in coords_set]
    return points

# print(shape(img1))

#copy and delete src folder from git
!git clone https://github.com/1adrianb/face-alignment
from distutils.dir_util import copy_tree
copy_tree("face-alignment", "")
import shutil
shutil.rmtree('face-alignment')

#get modl
import face_alignment
from skimage import io

fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)
fa3D = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False)

land2D = []
frame = []
preds_img1 = fa.get_landmarks(img1)

# get pose2D from face_alignment and 
# return normal pose2D
def normal_pose2D(pose2D):
  l1 = list(zip(*pose2D))[0]
  l2 = list(zip(*pose2D))[1]
  l1 = np.asarray(l1)
  l1 = l1/450
  l2 = np.asarray(l2)
  l2 = l2/450
  land_pred = np.asarray(list(zip(l1, l2)))
  return land_pred

def calc_diff(pose2D_1, pose2D_2):
  return abs((pose2D_1 - pose2D_2)).sum()

preds_img1_ = preds_img1[0];
# print(preds_img1_[0])
# print(ladmarks_2D_1[0])
l1 = list(zip(*preds_img1_))[0]
l2 = list(zip(*preds_img1_))[1]
# print(type(l1))
l1 = np.asarray(l1)
l1 = l1/450
l2 = np.asarray(l2)
l2 = l2/450
land_pred = np.asarray(list(zip(l1, l2)))
# print(land_pred)
print(type(land_pred))
print(type(ladmarks_2D_1))

print(land_pred)
print(ladmarks_2D_1)
land2D = []
land2D_get = [] 
land2D.append(('aa',land_pred))
land2D_get.append(('aa',ladmarks_2D_1))
error, listE = calc_NME(land2D, land2D_get)
meanError = error/len(listE)
# print(meanError)

print(calc_diff(ladmarks_2D_1, land_pred))

#create a network
# 10,000 landmarks images : pose(x,y,z)

#create a data:
iterat_ds = tfds.as_numpy(ds)
net_index_img_train = []
net_index_img_test = []
net_land_pose_train = []
net_land_pose_test = []
for i in range(1000):
  data = next(iterat_ds)
  land_ = data.get('landmarks_2d').flatten()
  pose_ = data.get('pose_params')[:3]
  data_ = [*land_,*pose_]
  net_land_pose_train.append(data_)
  net_index_img_train.append((i,data.get('image')))

for i in range(100):
  data = next(iterat_ds)
  land_ = data.get('landmarks_2d').flatten()
  pose_ = data.get('pose_params')[:3]
  data_ = [*land_,*pose_]
  net_land_pose_test.append(data_)
  net_index_img_test.append((i,data.get('image')))

print(len(net_land_pose_train))
print(len(net_index_img_test))

print(net_land_pose_train[0])

col = [] 
for i in range(68):
  col.append('x'+str(i))
for i in range(68):
  col.append('y'+str(i))

col.append('pitch_rad')
col.append('yaw_rad')
col.append('roll_rad')

# Import pandas library 
import pandas as pd 

# Create the pandas DataFrame 
df = pd.DataFrame(net_land_pose_train, columns = col) 
  
# print dataframe. 
print(df)

train_dataset = df.sample(frac=0.8,random_state=0)
test_dataset = df.drop(train_dataset.index)

train_dataset = train_dataset.reset_index()
test_dataset = test_dataset.reset_index()
train_dataset.pop('index')
test_dataset.pop('index')
print(train_dataset)
print(test_dataset)

train_data_table = train_dataset
train_pitch_rad = train_data_table.pop("pitch_rad")
train_yaw_rad = train_data_table.pop("yaw_rad")
train_roll_rad = train_data_table.pop("roll_rad")
train_labels_concat = pd.concat([train_pitch_rad, train_yaw_rad,train_roll_rad], axis=1).reset_index()
train_labels_concat.pop('index')

test_data_table = test_dataset
test_pitch_rad = test_data_table.pop("pitch_rad")
test_yaw_rad = test_data_table.pop("yaw_rad")
test_roll_rad = test_data_table.pop("roll_rad")
test_labels_concat = pd.concat([test_pitch_rad, test_yaw_rad,test_roll_rad], axis=1).reset_index()
test_labels_concat.pop('index')

print(train_data_table)
print( train_labels_concat)

print(test_data_table)
print( test_labels_concat)

def build_model():
  model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=[len(train_data_table.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(3)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model

from tensorflow import keras
from tensorflow.keras import layers
model = build_model()

model.summary()

# example_batch = train_data_table[:200]
# example_result = model.predict(example_batch)
# example_result

# !pip install git+https://github.com/tensorflow/docs
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling
EPOCHS = 1000

history = model.fit(
  train_data_table, train_labels_concat,
  epochs=EPOCHS, validation_split = 0.2, verbose=0,
  callbacks=[tfdocs.modeling.EpochDots()])



hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
plotter.plot({'Basic': history}, metric = "mean_absolute_error")
plt.ylim([0, 1])
plt.ylabel('MAE [MPG]')

model = build_model()

# The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)

early_history = model.fit( train_data_table, train_labels_concat,
                    epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])

loss, mae, mse = model.evaluate(test_data_table, test_labels_concat, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))

test_predictions = model.predict(test_data_table)

print(test_predictions)
print(test_labels_concat)

check = test_data_table[:2]
print(check)

test_predictions = model.predict(check)
print(test_predictions)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from skimage import io
import collections
input_img = img1
preds = fa3D.get_landmarks(input_img)[-1]

# 2D-Plot
plot_style = dict(marker='o',
                  markersize=4,
                  linestyle='-',
                  lw=2)

pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])
pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),
              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),
              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),
              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),
              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),
              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),
              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),
              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),
              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))
              }

fig = plt.figure(figsize=plt.figaspect(.5))
ax = fig.add_subplot(1, 2, 1)
ax.imshow(input_img)
for pred_type in pred_types.values():
    ax.plot(preds[pred_type.slice, 0],
            preds[pred_type.slice, 1],
            color=pred_type.color, **plot_style)

ax.axis('off')

# 3D-Plot
ax = fig.add_subplot(1, 2, 2, projection='3d')
surf = ax.scatter(preds[:, 0] * 1.2,
                  preds[:, 1],
                  preds[:, 2],
                  c='cyan',
                  alpha=1.0,
                  edgecolor='b')

for pred_type in pred_types.values():
    ax.plot3D(preds[pred_type.slice, 0] * 1.2,
              preds[pred_type.slice, 1],
              preds[pred_type.slice, 2], color='blue')

ax.view_init(elev=90., azim=90.)
ax.set_xlim(ax.get_xlim()[::-1])
plt.show()

